{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1noFJxlif0PG0QwQU4Jl6xmxQXUZHy-B7",
      "authorship_tag": "ABX9TyNxZCZ8aO4+wpqxZu5obvQM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adityasharma10699/demopygit/blob/main/Tweet_SVC%2BNBC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vD-HxBD4tBd",
        "outputId": "10f92fcb-9590-4590-d2a0-95acb641cf8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import re, nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import joblib"
      ],
      "metadata": {
        "id": "IpdBJPVq4_r3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading dataset as dataframe\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Data Mining/Class work/Tweet_Data.csv\")\n",
        "pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells\n",
        "pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window\n"
      ],
      "metadata": {
        "id": "5sUTA_Mm5Dlq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Tweets\n",
        "def cleaner(tweet):\n",
        "    soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n",
        "    souped = soup.get_text()\n",
        "    re1 = re.sub(r\"(@|http://|https://|www|\\\\x)\\S*\", \" \", souped) # substituting @mentions, urls, etc with whitespace\n",
        "    re2 = re.sub(\"[^A-Za-z]+\",\" \", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace\n",
        "\n",
        "    \"\"\"\n",
        "    For more info on regular expressions visit -\n",
        "    https://docs.python.org/3/howto/regex.html\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = nltk.word_tokenize(re2)\n",
        "    lower_case = [t.lower() for t in tokens]\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n",
        "\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n",
        "    return lemmas\n",
        "\n",
        "df['cleaned_tweet'] = df.full_text.apply(cleaner)\n",
        "df = df[df['cleaned_tweet'].map(len) > 0] # removing rows with cleaned tweets of length 0\n",
        "print(\"Printing top 5 rows of dataframe showing original and cleaned tweets....\")\n",
        "print(df[['full_text','cleaned_tweet']].head())\n",
        "df.drop(['id', 'created_at', 'full_text'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKE0DMwG5N-F",
        "outputId": "4ef54f6f-6bc5-4019-f9b7-c7b84085c291"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-dd5854438e25>:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(tweet, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing top 5 rows of dataframe showing original and cleaned tweets....\n",
            "                                                                                                                                                                                                                                                                                                                           full_text  \\\n",
            "0                                                                                                                                                              b\"@twolfuk wow, that was quiet a dramatic tale &gt;.&gt; i don't think anyone knows that Nick and Judy have been living in our back garden for the last few years :P\"   \n",
            "1                                                                                                    b\"No stream this weekend i'm going to a music festival for 3 days, but i will be back with some more #Battlefield Monday 11pm uk time, i hope you all have a Great Weekend :) #BestCommunityInTheWorld https://t.co/E1htQmhJpf\"   \n",
            "2  b\"yes it is true, a stream schedule so you know what to expect from me :D this is a general solution to timeliness and I plan on abiding by it as often as I can to appease the golf-needing masses. Please forgive me if I don't manage it every day! I'm trying my best on this grind \\xf0\\x9f\\x98\\x87 https://t.co/kwpcQIiQ3U\"   \n",
            "3                                                                             b\"@ThorntonParsons @1776Stonewall You know they called Mueller a national treasure. Lol. Maybe he symbolizes a treasure chest....I'm all for opening that treasure chest and counting a compatible sentence. You know like Manaforts.  :-). Big Time!\"   \n",
            "4                                                                                                                                                                   b\"@CalamityDeath I honestly barely play too lol it's not a big deal imo. but art or not, I think as long as you enjoy the game, you belong in the community :))\"   \n",
            "\n",
            "                                                                                                                                                                                 cleaned_tweet  \n",
            "0                                                                                        [b, wow, quiet, dramatic, tale, think, anyone, know, nick, judy, living, back, garden, last, year, p]  \n",
            "1                                                    [b, stream, weekend, going, music, festival, day, back, battlefield, monday, pm, uk, time, hope, great, weekend, bestcommunityintheworld]  \n",
            "2  [b, yes, true, stream, schedule, know, expect, general, solution, timeliness, plan, abiding, often, appease, golf, needing, mass, please, forgive, manage, every, day, trying, best, grind]  \n",
            "3          [b, know, called, mueller, national, treasure, lol, maybe, symbolizes, treasure, chest, opening, treasure, chest, counting, compatible, sentence, know, like, manaforts, big, time]  \n",
            "4                                                                                           [b, honestly, barely, play, lol, big, deal, imo, art, think, long, enjoy, game, belong, community]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving cleaned tweets to csv\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "df['cleaned_tweet'] = [\" \".join(row) for row in df['cleaned_tweet'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input\n",
        "data = df['cleaned_tweet']\n",
        "Y = df['sentiment'] # target column\n",
        "tfidf = TfidfVectorizer(min_df=.00015, ngram_range=(1,3)) # min_df=.00015 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering\n",
        "tfidf.fit(data) # learn vocabulary of entire data\n",
        "data_tfidf = tfidf.transform(data) # creating tfidf values\n",
        "pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)\n",
        "print(\"Shape of tfidf matrix: \", data_tfidf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHTFeIUu5UIM",
        "outputId": "8a7d90ca-7dc5-40dc-ec23-9af5b26126ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tfidf matrix:  (200000, 13383)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Support Vector Classifier\n",
        "svc_clf = LinearSVC() # kernel = 'linear' and C = 1\n"
      ],
      "metadata": {
        "id": "g2ZXIVAa5XXc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "scores=[]\n",
        "iteration = 0\n",
        "for train_index, test_index in kf.split(data_tfidf, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "    X_train, Y_train = data_tfidf[train_index], Y[train_index]\n",
        "    X_test, Y_test = data_tfidf[test_index], Y[test_index]\n",
        "\n",
        "    svc_clf.fit(X_train, Y_train) # Fitting SVC\n",
        "    Y_pred = svc_clf.predict(X_test)\n",
        "    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n",
        "    print(\"Cross-validation accuracy: \", score)\n",
        "    scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "svc_mean_accuracy = np.mean(scores)\n",
        "print(\"Mean cross-validation accuracy: \", svc_mean_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4eOZcam5Z_a",
        "outputId": "d54d7dcc-f335-42b4-bf3e-074e68e98e40"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1\n",
            "Cross-validation accuracy:  0.794\n",
            "Iteration  2\n",
            "Cross-validation accuracy:  0.7959\n",
            "Iteration  3\n",
            "Cross-validation accuracy:  0.80035\n",
            "Iteration  4\n",
            "Cross-validation accuracy:  0.7887\n",
            "Iteration  5\n",
            "Cross-validation accuracy:  0.7941\n",
            "Iteration  6\n",
            "Cross-validation accuracy:  0.79625\n",
            "Iteration  7\n",
            "Cross-validation accuracy:  0.7923\n",
            "Iteration  8\n",
            "Cross-validation accuracy:  0.79415\n",
            "Iteration  9\n",
            "Cross-validation accuracy:  0.79465\n",
            "Iteration  10\n",
            "Cross-validation accuracy:  0.79195\n",
            "Mean cross-validation accuracy:  0.794235\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing Naive Bayes Classifier\n",
        "nbc_clf = MultinomialNB()\n"
      ],
      "metadata": {
        "id": "R_DBso3e5e9q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running cross-validation\n",
        "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "scores=[]\n",
        "iteration = 0\n",
        "for train_index, test_index in kf.split(data_tfidf, Y):\n",
        "    iteration += 1\n",
        "    print(\"Iteration \", iteration)\n",
        "    X_train, Y_train = data_tfidf[train_index], Y[train_index]\n",
        "    X_test, Y_test = data_tfidf[test_index], Y[test_index]\n",
        "    nbc_clf.fit(X_train, Y_train) # Fitting NBC\n",
        "    Y_pred = nbc_clf.predict(X_test)\n",
        "    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy\n",
        "    print(\"Cross-validation accuracy: \", score)\n",
        "    scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "nbc_mean_accuracy = np.mean(scores)\n",
        "print(\"Mean cross-validation accuracy: \", nbc_mean_accuracy)\n",
        "\n",
        "if svc_mean_accuracy > nbc_mean_accuracy:\n",
        "  clf = LinearSVC().fit(data_tfidf, Y)\n",
        "  joblib.dump(clf, 'svc.sav')\n",
        "else:\n",
        "  clf = MultinomialNB().fit(data_tfidf, Y)\n",
        "  joblib.dump(clf, 'nbc.sav')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RfZ9Unb5hha",
        "outputId": "ad8462bb-6505-4f07-e882-896d7adc8ffa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration  1\n",
            "Cross-validation accuracy:  0.777\n",
            "Iteration  2\n",
            "Cross-validation accuracy:  0.774\n",
            "Iteration  3\n",
            "Cross-validation accuracy:  0.77325\n",
            "Iteration  4\n",
            "Cross-validation accuracy:  0.7709\n",
            "Iteration  5\n",
            "Cross-validation accuracy:  0.7739\n",
            "Iteration  6\n",
            "Cross-validation accuracy:  0.7775\n",
            "Iteration  7\n",
            "Cross-validation accuracy:  0.7729\n",
            "Iteration  8\n",
            "Cross-validation accuracy:  0.7728\n",
            "Iteration  9\n",
            "Cross-validation accuracy:  0.7728\n",
            "Iteration  10\n",
            "Cross-validation accuracy:  0.7708\n",
            "Mean cross-validation accuracy:  0.7735850000000001\n"
          ]
        }
      ]
    }
  ]
}